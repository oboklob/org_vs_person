# ISO20275 Filter Accuracy Analysis

## Overview

This document analyzes the **accuracy** of the ISO20275 legal form filter compared to the ML classifier. While the [performance analysis](FILTER_PERFORMANCE_ANALYSIS.md) showed that the filter is slower, the accuracy analysis reveals **the filter provides superior precision**.

## Executive Summary

**Key Finding**: The ISO20275 filter achieves **99.35-99.97% precision** when detecting organizations, compared to the ML classifier's **96.34% precision**. However, the filter has much lower recall (28-54%) vs the classifier's 92% recall.

### Trade-off Summary

| Metric | ISO20275 Filter (Tier A) | ISO20275 Filter (All Tiers) | ML Classifier |
|--------|--------------------------|----------------------------|---------------|
| **Precision** | 99.97% | 99.35% | 96.34% |
| **Recall** | 28.67% | 53.73% | 92.28% |
| **F1-Score** | 0.45 | 0.70 | 0.94 |
| **Coverage** | 14.1% | 26.6% | 100% |

## Detailed Benchmark Results

### Test Configuration
- **Dataset**: 50,000 samples from test set
- **Ground Truth**: Human-labeled PER/ORG classifications
- **Tier Configurations**: Tier A only, Tier A+B, All tiers (A, B, C)

### Tier A Only (Most Conservative)

> [!NOTE]
> Highest precision, lowest coverage

**Performance Metrics:**
- **Coverage**: 14.1% (7,048 detections out of 50,000 samples)
- **Precision**: **99.97%** (7,046 correct, 2 false positives)
- **Recall**: 28.67% (detected 7,046 out of 24,574 true ORGs)
- **F1-Score**: 0.4456

**Key Insights:**
- Only 2 false positives in 7,048 detections
- Catches ~29% of all organizations with near-perfect accuracy
- Classifier agrees with filter 99.9% of the time on these cases

### Tier A+B (Recommended)

> [!NOTE]
> Best balance of precision and coverage

**Performance Metrics:**
- **Coverage**: 25.6% (12,792 detections)
- **Precision**: **99.59%** (12,739 correct, 53 false positives)
- **Recall**: 51.84% (detected 12,739 out of 24,574 true ORGs)
- **F1-Score**: 0.6818

**Key Insights:**
- Only 53 false positives in 12,792 detections
- Catches ~52% of all organizations with very high accuracy
- Classifier agrees with filter 99.5% of the time

### All Tiers (A, B, C)

> [!WARNING]
> Includes ambiguous short forms (Tier C) which reduce precision slightly

**Performance Metrics:**
- **Coverage**: 26.6% (13,290 detections)
- **Precision**: **99.35%** (13,203 correct, 87 false positives)
- **Recall**: 53.73% (detected 13,203 out of 24,574 true ORGs)
- **F1-Score**: 0.6974

**Key Insights:**
- Only 87 false positives in 13,290 detections
- Marginally better recall than A+B but slightly lower precision
- Most false positives come from Tier C ambiguous forms

## Comparison with ML Classifier

### ML Classifier Performance
- **Overall Accuracy**: 94.48%
- **ORG Precision**: 96.34%
- **ORG Recall**: 92.28%
- **ORG F1-Score**: 0.9427

### Direct Comparison

#### Precision (Accuracy when predicting ORG)

| Approach | Precision | False Positives (out of ~13k predictions) |
|----------|-----------|-------------------------------------------|
| ISO20275 Filter (All tiers) | **99.35%** | 87 |
| ML Classifier | 96.34% | ~900* |

*Extrapolated from overall metrics

> [!IMPORTANT]
> **The filter makes 3-4pp fewer errors** when it predicts ORG, which translates to roughly **10x fewer false positives** in its detection range.

#### Recall (Coverage of true organizations)

| Approach | Recall | Organizations Missed |
|----------|--------|---------------------|
| ISO20275 Filter (Tier A) | 28.67% | 71.33% |
| ISO20275 Filter (All tiers) | 53.73% | 46.27% |
| ML Classifier | **92.28%** | 7.72% |

The classifier catches far more organizations overall.

#### Agreement Analysis

When the filter detects an organization:
- Classifier **agrees** (also says ORG): 99.1-99.9%
- Classifier is **correct** on these cases: 99.71-99.91%

This high agreement suggests:
1. Filter detections are "easy cases" that the model also gets right
2. Filter's additional precision (99.35% vs 96.34%) comes from avoiding edge cases where the model fails

## Use Case Recommendations

### When to Use ISO20275 Filter

#### âœ“ High-Precision Workflows
- **Automated processing** where false positives are costly
- **Compliance/regulatory** use cases requiring audit trails
- **Data quality** where you need guaranteed accuracy

Example: Automatically flagging entities for sanctions screening
```python
# Only flag with very high confidence
detector = FastOrgDetector(tier_filter=['A'])  # 99.97% precision
orgs, unknown = detector.filter_orgs(names)
# Process `orgs` automatically, manual review for `unknown`
```

#### âœ“ Explainability Requirements
- **User-facing** applications where you need to explain classifications
- **Audit trails** requiring clear reasoning
- **Legal/compliance** documentation

Example: Showing users why something was classified
```python
result = classifier.classify_with_diagnostics(name)
if result.reason_codes['matched_legal_form']:
    print(f"Detected legal form: {result.reason_codes['matched_legal_form']}")
```

#### âœ“ Two-Stage Classification
- **Pre-filter** high-confidence cases, use ML for uncertain ones
- **Cost optimization** where ML inference is expensive
- **Hybrid approach** combining rules + ML

Example: Reduce ML API calls
```python
# Stage 1: Fast filter (local, free)
detector = FastOrgDetector(tier_filter=['A', 'B'])
orgs, uncertain = detector.filter_orgs(names)

# Stage 2: ML only for uncertain cases (API calls, paid)
uncertain_predictions = expensive_ml_api.classify(uncertain)
```

### When to Use ML Classifier

#### âœ“ Maximum Coverage
- **Comprehensive classification** where you need results for everything
- **High recall** requirements (catch as many ORGs as possible)
- **Batch processing** where performance matters (see [performance analysis](FILTER_PERFORMANCE_ANALYSIS.md))

Example: Classify entire database
```python
# Simple, fast, comprehensive
predictions = classifier.classify_list(all_names)
```

#### âœ“ Balanced Precision/Recall
- **General-purpose** classification
- **F1 optimization** (harmonic mean of precision/recall)
- **No special precision requirements**

The classifier's 96.34% precision is still very good for most use cases.

## Precision by Tier Analysis

Understanding what each tier catches:

### Tier A Forms
- **Examples**: "Limited", "Corporation", "GmbH", "SociÃ©tÃ© Anonyme"
- **Characteristics**: Multi-word forms or clear long abbreviations (â‰¥4 chars)
- **Precision**: 99.97%
- **Why accurate**: Unlikely to appear in person names
- **False positives**: Typically edge cases like "John Limited" (rare personal naming)

### Tier B Forms
- **Examples**: "LLC", "plc", "SpA", "NV" (but only unambiguous ones)
- **Characteristics**: Short forms (2-3 chars) not in ambiguous list
- **Precision**: ~99.7% (estimated from A+B vs A-only)
- **Why accurate**: Curated to exclude person-name collisions
- **False positives**: Some personal name suffixes in specific languages

### Tier C Forms
- **Examples**: "SA", "AG", "AB", "AS" (potential person name collisions)
- **Characteristics**: Ambiguous short forms that could be personal names
- **Precision**: ~97% (estimated from All tiers vs A+B)
- **Why less accurate**: "Martinez SA" could be a person or company in Spanish
- **False positives**: Personal names that end with these forms

## Statistical Significance

With 50,000 samples:
- **Filter (All tiers)**: 13,290 predictions, 87 errors = 99.35% Â± 0.14% (95% CI)
- **Classifier**: ~24,600 ORG predictions, ~900 errors = 96.34% Â± 0.24% (95% CI)

The 3pp difference is **statistically significant** (p < 0.001).

## Conclusion

### The Trade-off

| Feature | ISO20275 Filter | ML Classifier | Winner |
|---------|----------------|---------------|---------|
| **Precision** | 99.35-99.97% | 96.34% | ðŸ† Filter |
| **Recall** | 28-54% | 92.28% | ðŸ† Classifier |
| **Coverage** | 27% | 100% | ðŸ† Classifier |
| **Speed** | Slower (batch) | Faster (batch) | ðŸ† Classifier |
| **Explainability** | High | Low | ðŸ† Filter |
| **F1-Score** | 0.45-0.70 | 0.94 | ðŸ† Classifier |

### Recommendations

> [!IMPORTANT]
> **Primary Recommendation**: Use ML classifier as default, use filter for special cases

1. **For general-purpose classification**: Use the ML classifier
   - Better balance of precision/recall
   - Faster for batch processing
   - Simpler code path

2. **For high-precision requirements**: Use ISO20275 filter with Tier A or A+B
   - 3-4pp better precision
   - ~10x fewer false positives in detection range
   - Clear audit trail with legal form identification

3. **For hybrid approach**: Two-stage classification
   - Stage 1: Filter catches high-confidence ORGs (Tier A+B)
   - Stage 2: Classifier handles remaining cases
   - Benefits: Explainability for ~25% of cases + comprehensive coverage

### Perfect Use Cases for Filter

The filter excels when:
- âœ“ **False positives are expensive** (e.g., wrongly blocking a person)
- âœ“ **Explainability is required** (e.g., regulatory compliance)
- âœ“ **Partial coverage is acceptable** (e.g., flagging obvious cases)
- âœ“ **High precision > high recall** (e.g., automated actions)

The classifier excels when:
- âœ“ **Complete coverage needed** (e.g., must classify everything)
- âœ“ **Performance matters** (e.g., batch processing millions)
- âœ“ **Balanced precision/recall** (e.g., general analytics)
- âœ“ **Simplicity preferred** (e.g., straightforward pipeline)

## Benchmark Scripts

### Accuracy Benchmark
```bash
# Single configuration
python scripts/benchmark_filter_accuracy.py --size 50000

# Compare tier configurations
python scripts/benchmark_filter_accuracy.py --size 50000 --compare-tiers

# Tier A only
python scripts/benchmark_filter_accuracy.py --size 50000 --tier-filter A

# Tier A+B
python scripts/benchmark_filter_accuracy.py --size 50000 --tier-filter A B
```

### Performance Benchmark
```bash
python scripts/benchmark_filter_performance.py --sizes 1000 10000 50000 --runs 5
```

See [FILTER_PERFORMANCE_ANALYSIS.md](FILTER_PERFORMANCE_ANALYSIS.md) for performance details.
